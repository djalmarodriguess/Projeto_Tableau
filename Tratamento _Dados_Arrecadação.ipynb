{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importar bibliotecas"
      ],
      "metadata": {
        "id": "Qa9mtc20bOx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import warnings\n",
        "\n",
        "# Ignora avisos\n",
        "warnings.simplefilter(action='ignore')\n",
        "\n",
        "# Define a exibição de floats com apenas duas casas decimais\n",
        "pd.options.display.float_format = '{:.2f}'.format"
      ],
      "metadata": {
        "id": "gOcjZLLlUBaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importação dos Dados"
      ],
      "metadata": {
        "id": "pRhEoehmbeKb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-9AwMLs-QM7"
      },
      "outputs": [],
      "source": [
        "# Caminho da pasta com os arquivos\n",
        "caminho_pasta = \"/content/drive/MyDrive/ARR VS FAT - 22\"\n",
        "\n",
        "# Obtenha todos os arquivos Excel na pasta\n",
        "arquivos_excel = glob(os.path.join(caminho_pasta, \"*.xlsx\"))\n",
        "\n",
        "# Lista para armazenar cada DataFrame lido\n",
        "dataframes = []\n",
        "\n",
        "# Leia cada arquivo Excel e adicione à lista\n",
        "for arquivo in arquivos_excel:\n",
        "    df = pd.read_excel(arquivo)\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Concatene todos os DataFrames\n",
        "df1_concatenado = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Exiba as primeiras linhas do DataFrame concatenado para verificar\n",
        "df1_concatenado.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho da pasta com os arquivos\n",
        "caminho_pasta = \"/content/drive/MyDrive/ARR VS FAT - 23\"\n",
        "\n",
        "# Obtenha todos os arquivos Excel na pasta\n",
        "arquivos_excel = glob(os.path.join(caminho_pasta, \"*.xlsx\"))\n",
        "\n",
        "# Lista para armazenar cada DataFrame lido\n",
        "dataframes = []\n",
        "\n",
        "# Leia cada arquivo Excel e adicione à lista\n",
        "for arquivo in arquivos_excel:\n",
        "    df = pd.read_excel(arquivo)\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Concatene todos os DataFrames\n",
        "df2_concatenado = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Exiba as primeiras linhas do DataFrame concatenado para verificar\n",
        "df2_concatenado.head()\n"
      ],
      "metadata": {
        "id": "mpYU55KsBjVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho da pasta com os arquivos\n",
        "caminho_pasta = \"/content/drive/MyDrive/ARR VS FAT - 24\"\n",
        "\n",
        "# Obtenha todos os arquivos Excel na pasta\n",
        "arquivos_excel = glob(os.path.join(caminho_pasta, \"*.xlsx\"))\n",
        "\n",
        "# Lista para armazenar cada DataFrame lido\n",
        "dataframes = []\n",
        "\n",
        "# Leia cada arquivo Excel e adicione à lista\n",
        "for arquivo in arquivos_excel:\n",
        "    df = pd.read_excel(arquivo)\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Concatene todos os DataFrames\n",
        "df3_concatenado = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Exiba as primeiras linhas do DataFrame concatenado para verificar\n",
        "df3_concatenado.head()\n"
      ],
      "metadata": {
        "id": "hQXpTNB2BjYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir os dataframes\n",
        "df_final = pd.concat([df1_concatenado,df2_concatenado,df3_concatenado], ignore_index=True)\n",
        "df_final.head()"
      ],
      "metadata": {
        "id": "w_cqCnkVXVbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.shape"
      ],
      "metadata": {
        "id": "vROfOJeOXWQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tranformação"
      ],
      "metadata": {
        "id": "ymsmdsvETmr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria uma cópia do DataFrame original para não alterar o original\n",
        "df = df_final.copy()"
      ],
      "metadata": {
        "id": "Sic8bsoBXWSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limpeza dos dados"
      ],
      "metadata": {
        "id": "Kd3x-re8kHQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui valores ausentes na coluna \"Tipo_Fatura\" por \"Impressa\"\n",
        "df[\"Tipo_Fatura\"].fillna(\"Impressa\", inplace = True)\n"
      ],
      "metadata": {
        "id": "Ohb6GHaGYHZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui valores ausentes na coluna \"Segmento\" por \"Controle\"\n",
        "df[\"Segmento\"].fillna(\"Controle\", inplace = True)\n"
      ],
      "metadata": {
        "id": "kt6m2tPLYHc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preenche valores ausentes na coluna \"TOTAL_VLR\" com a média dos valores da coluna\n",
        "df[\"TOTAL_VLR\"].fillna(df[\"TOTAL_VLR\"].mean(), inplace = True)\n"
      ],
      "metadata": {
        "id": "DYYGzGDSYHeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manejo de variáveis categóricas"
      ],
      "metadata": {
        "id": "7oLERKwUkcIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte a coluna \"TOTAL_VLR\" para o tipo float e arredonda para duas casas decimais\n",
        "df['TOTAL_VLR'] = df['TOTAL_VLR'].astype(float).round(2)"
      ],
      "metadata": {
        "id": "q1cLK0UIlf6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui \"NI\" por \"M+1\" na coluna \"AGING_RESUMIDO\"\n",
        "df[\"AGING_RESUMIDO\"].replace(\"NI\", \"M+1\", inplace = True)"
      ],
      "metadata": {
        "id": "Bs6AOu2_YHgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria a coluna \"método_resumido\" para categorizar os métodos de pagamento\n",
        "df[\"método_resumido\"] = df[\"Método\"].apply(lambda x: x if x in [\"PIX\", \"Internet\", \"Débito Autom.\", \"Lotérica\", \"Eletrônico\", \"PEC\"] else \"Outros\")\n"
      ],
      "metadata": {
        "id": "w08IMKKE7EXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dicionário de mapeamento para as faixas de vencimento\n",
        "mapping = {\n",
        "    \"0 - Antes do vencimento\": \"Antes do vencimento\",\n",
        "    \"1 - No vencimento\": \"No vencimento\",\n",
        "    \"2 - até 05 dias\": \"de 1 à 3 dias depois\",\n",
        "    \"3 - até 10 dias\": \"de 4 à 10 dias depois\",\n",
        "    \"4 - até 20 dias\": \"de 11 à 19 dias depois\",\n",
        "    \"5 - até 30 dias\": \"de 20 à 30 dias depois\",\n",
        "    \"6 - 31 até 60 dias\": \"de 31 à 60 dias depois\",\n",
        "    \"7 - 61 até 90 dias\": \"de 61 à 90 dias depois\",\n",
        "    \"8 - > 90 dias\": \"Após 90 dias\"\n",
        "}\n",
        "\n",
        "# Aplica o mapeamento à coluna \"AGING_PAGAMENTO\"\n",
        "df[\"AGING_PAGAMENTO\"] = df[\"AGING_PAGAMENTO\"].replace(mapping)\n"
      ],
      "metadata": {
        "id": "ecO8YemVJM4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os valores específicos por \"Live\"\n",
        "df[\"Segmento\"] = df[\"Segmento\"].replace({\"Fixo\": \"Live\", \"VOIP\": \"Live\", \"WTTX\": \"Live\"})"
      ],
      "metadata": {
        "id": "h70pNbyiMQ8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui o valor \"Fatura não encontrada\" por \"SP\" na coluna \"UF\"\n",
        "df[\"UF\"].replace(\"Fatura não encontrada\", \"SP\", inplace = True)"
      ],
      "metadata": {
        "id": "X7biNdhUMQ_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preenche valores ausentes na coluna \"UF\" com \"SP\"\n",
        "df[\"UF\"].fillna(\"SP\", inplace = True)"
      ],
      "metadata": {
        "id": "j311Zgs8Zi_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enriquecimentos dos dados"
      ],
      "metadata": {
        "id": "1b-W7EG_kNvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista dos feriados nacionais para 2022, 2023 e 2024\n",
        "feriados = [\n",
        "    \"01/01/2022\", \"01/03/2022\", \"15/04/2022\", \"21/04/2022\", \"01/05/2022\",\n",
        "    \"16/06/2022\", \"07/09/2022\", \"12/10/2022\", \"02/11/2022\", \"15/11/2022\", \"25/12/2022\",\n",
        "    \"01/01/2023\", \"20/02/2023\", \"21/02/2023\", \"07/04/2023\", \"21/04/2023\",\n",
        "    \"01/05/2023\", \"08/06/2023\", \"07/09/2023\", \"12/10/2023\", \"02/11/2023\",\n",
        "    \"15/11/2023\", \"25/12/2023\", \"01/01/2024\", \"12/02/2024\", \"13/02/2024\",\n",
        "    \"29/03/2024\", \"21/04/2024\", \"01/05/2024\", \"30/05/2024\", \"07/09/2024\",\n",
        "    \"12/10/2024\", \"02/11/2024\", \"15/11/2024\", \"20/11/2024\", \"25/12/2024\"\n",
        "]\n",
        "# Converte a lista de feriados para o tipo datetime\n",
        "feriados = pd.to_datetime(feriados, format=\"%d/%m/%Y\")"
      ],
      "metadata": {
        "id": "gB_woUep5tvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria a coluna \"WD\" com o dia da semana, onde segunda-feira é 0 e domingo é 6\n",
        "df[\"WD\"] = df[\"Data_PG\"].dt.weekday"
      ],
      "metadata": {
        "id": "6pe-_bNJ1YUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria a coluna \"DU\" para verificar se é dia útil\n",
        "# Dia útil é segunda a sexta (0 a 4) e não deve ser feriado\n",
        "df[\"DU\"] = df.apply(lambda row: 1 if row[\"WD\"] < 5 and row[\"Data_PG\"] not in feriados else 0, axis=1)"
      ],
      "metadata": {
        "id": "26DKWRBe1YWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dicionário de mapeamento das siglas para o nome completo dos estados\n",
        "estado_nome_completo = {\n",
        "    'AC': 'Acre', 'AL': 'Alagoas', 'AM': 'Amazonas', 'AP': 'Amapá',\n",
        "    'BA': 'Bahia', 'CE': 'Ceará', 'DF': 'Distrito Federal', 'ES': 'Espírito Santo',\n",
        "    'GO': 'Goiás', 'MA': 'Maranhão', 'MG': 'Minas Gerais', 'MS': 'Mato Grosso do Sul',\n",
        "    'MT': 'Mato Grosso', 'PA': 'Pará', 'PB': 'Paraíba', 'PE': 'Pernambuco',\n",
        "    'PI': 'Piauí', 'PR': 'Paraná', 'RJ': 'Rio de Janeiro', 'RN': 'Rio Grande do Norte',\n",
        "    'RO': 'Rondônia', 'RR': 'Roraima', 'RS': 'Rio Grande do Sul', 'SC': 'Santa Catarina',\n",
        "    'SE': 'Sergipe', 'SP': 'São Paulo', 'TO': 'Tocantins'\n",
        "}\n",
        "\n",
        "# Cria a nova coluna com o nome completo dos estados\n",
        "df['Estado'] = df['UF'].map(estado_nome_completo)"
      ],
      "metadata": {
        "id": "A1TOqnS_ZjCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dicionário de mapeamento das siglas para as regiões\n",
        "estado_regiao = {\n",
        "    'AC': 'Norte', 'AL': 'Nordeste', 'AM': 'Norte', 'AP': 'Norte', 'BA': 'Nordeste',\n",
        "    'CE': 'Nordeste', 'DF': 'Centro-Oeste', 'ES': 'Sudeste', 'GO': 'Centro-Oeste',\n",
        "    'MA': 'Nordeste', 'MG': 'Sudeste', 'MS': 'Centro-Oeste', 'MT': 'Centro-Oeste',\n",
        "    'PA': 'Norte', 'PB': 'Nordeste', 'PE': 'Nordeste', 'PI': 'Nordeste', 'PR': 'Sul',\n",
        "    'RJ': 'Sudeste', 'RN': 'Nordeste', 'RO': 'Norte', 'RR': 'Norte', 'RS': 'Sul',\n",
        "    'SC': 'Sul', 'SE': 'Nordeste', 'SP': 'Sudeste', 'TO': 'Norte'\n",
        "}\n",
        "\n",
        "# Cria a nova coluna com a região correspondente a cada estado\n",
        "df['Região'] = df['UF'].map(estado_regiao)"
      ],
      "metadata": {
        "id": "coH1Eg94ZjES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adiciona a coluna 'País'\n",
        "df['País'] = 'Brasil'"
      ],
      "metadata": {
        "id": "CH9iTUSjZjF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Salva o DataFrame em um arquivo CSV\n",
        "df.to_csv(\"df_final.csv\", index=False)"
      ],
      "metadata": {
        "id": "DRluQiBJ-ADf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}